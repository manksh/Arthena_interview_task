{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arthena Data Science Challenge - Part 1.2 & Part 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Manksh Gupta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to train a model to predict the `hammer_price` of lots by Picasso at upcoming auctions (some upcoming lots are included in your dataset). Note that this means that you can't use future data to predict the past. You may use as many or as few features as you like except for `buyers_premium` (it's based on the sale price. See [SCHEMA.md](SCHEMA.md) for more details). Did you perform any data cleaning, filtering or transformations to improve the model fit? Why did you choose this model? What loss function did you use? Why did you pick this loss function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import preprocess_individual\n",
    "from utils import percentage_diff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first stage of the problem, this is also argubly the most important stage. Models are data dependent and if the data is garbage, the model will give us garbage. Thus, its important to preprocess the data, extract useful features and then train models on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "picasso = pd.read_csv('artists/picasso.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Preprocessing techniques that I am using are:\n",
    "\n",
    "1) Dealing with Missing Values/Incorrect Values  - The data has a lot of missing values denoted by '-1', wherever it makes sense, I replace this with the mean value and where that doesn't make sense, i either remove the enties or try some other missing data techniques, these are described in the comments.\n",
    "\n",
    "2) Adjusting Hammer Price - Hammer price is in different currencies, I convert everything to USD using the given conversion rates.\n",
    "\n",
    "3) Tring to integrate Time - Since there is a time component to te model, I try various techniques to incoporate that. First, I observed a monthly seasonality, thus i include auction month as a feature. I also take difference in years between auction and death, auction and work_execution etc. to see what affect this time has on the mode.\n",
    "\n",
    "4) Categorical features - There are a lot of features such as name, type of work, auction dept, painting vs sculpture etc that are converted to one-hot-encoded features in the model. \n",
    "\n",
    "5) Size/Area of the artwork is one feature that I would have loved to include, however, due to the missing nature of the measurement units, it is very hard to include this- The paintings are measured in 'cm', 'mm', 'in', 'm' but a lot of these are missing in the data nd thus its hard to include this feature. \n",
    "\n",
    "Finally, I drop the columns that I dont use and then go to the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_individual(picasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I do the following:\n",
    "\n",
    "1) Seperate the data where the hammer price is '-1'. There are two types of places where this happens, first when the lot didn't sell and when the auction is in the future. The instances when it didn't sell are later removed and the instances in the future are saves as a different dataset to predict on later.\n",
    "\n",
    "2) I then seperate the data into labels and data for making the model.\n",
    "\n",
    "3) I split the data into train and test. All artwork after 2017 is in the test and and all before is in the train set. This is different from how one would evaluate a model without any time component, here we are esentially using data upto 2017 to predict for the future.\n",
    "\n",
    "4) I finally create the'future' data, this is the data for future auctions that we want to predict with the best model that we find. Assuming, November 2018 and after is the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = df[df['adjusted_hammer_price'] < 0 ]\n",
    "df = df[df['adjusted_hammer_price'] > 0 ]\n",
    "label = df['adjusted_hammer_price']\n",
    "df = df.drop(columns = ['adjusted_hammer_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[df.auction_year < 2017]\n",
    "X_test = df[df.auction_year >= 2017]\n",
    "y_train = label[X_train.index]\n",
    "y_test = label[X_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = final_test[final_test.auction_year>=2018]\n",
    "future = future[future.auction_month>=11]\n",
    "future = future.drop(columns = ['adjusted_hammer_price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I try different models below, gridsearch hyperparemeters and then finally decide on the model that works best for this problem. For all the models, I am trying to optimize mean_squared_error. I tried optimizing mean_absolute_error as well(due to the fact that its less sensitive to outliers than mse), however, the results were comparable and the training time was significantly slower. This is probably due to the fact that mse has nicer mathematical properties(differentiable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following models that I try, I evaluate results using 2 main metrics - Rsquared and the percentage difference predictions have with the actual predictions. The first metric is fairly standard in industry and is one of the most popular metrics to evaluate regressions. The second metric however is a unique one. It essentially measures the percentage difference between actual and predicted values. Ideally, we want to choose the model where most observations have a small percentage difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the time series nature of the data, the training and testing of the model is done a little bit differently. Normally, one randomly shuffles the data and then trains on one part and tests on the other. Since we want to predict for the future, it make sense to test only on the past data and predict on future data. I split the data into train and test based on time(before and after 2017), then I trained on before 2017 and tested on after 2017. However, while doing cross validation, the data is randomly shuffeled and cross validated. This is fine as at a given point in time, we have all the past data and it makes sense to randomly cross validate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Squared Error is also an important metric that is tracked in regression problems, however I am not tracking that. This is because MSE is extremely sensitive to outliers. For example, If a painting was sold for 5 Million but we predict 4.5 million, the values are actuaally not that far but the MSE will be extremely high(difference of 500 Thousand squared). Thus, MSE may not be a good metric to track in this particuar problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression - Lasso Penalty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'alpha':[0.6,0.7,0.8,0.9,1,1.5,2,5,10,20,50,100,200,300,1000,2000,3000]}\n",
    "regressor = linear_model.Lasso(fit_intercept=True, max_iter=10000, tol = .1, random_state = 42)\n",
    "reg = GridSearchCV(regressor, parameters, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=10000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=42,\n",
       "   selection='cyclic', tol=0.1, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [0.6, 0.7, 0.8, 0.9, 1, 1.5, 2, 5, 10, 20, 50, 100, 200, 300, 1000, 2000, 3000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Linear Regression are: {'alpha': 2000}\n",
      "Linear Regression - Train R-squared 0.9170384011915328\n",
      "Linear Regression - Test R-squared 0.8793459122235444\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for Linear Regression are:', reg.best_params_)\n",
    "print('Linear Regression - Train R-squared', reg.score(X_train, y_train))\n",
    "print('Linear Regression - Test R-squared', reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10% or lesser Difference': 6825,\n",
       " '25% or lesser Difference': 151,\n",
       " '50% or lesser Difference': 26,\n",
       " '75% or lesser Difference': 5,\n",
       " 'Greater than or Equal to 100%': 4}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_diff(y_train, reg.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10% or lesser Difference': 974,\n",
       " '25% or lesser Difference': 8,\n",
       " '50% or lesser Difference': 2,\n",
       " '75% or lesser Difference': 1,\n",
       " 'Greater than or Equal to 100%': 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_diff(y_test, reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth':[5,6,7,8,10,12,15,20],'min_samples_split':[5,10,15,20,30,50]  }\n",
    "regressor = DecisionTreeRegressor(random_state=0, criterion='mse')\n",
    "reg = GridSearchCV(regressor, parameters, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=0, splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [5, 6, 7, 8, 10, 12, 15, 20], 'min_samples_split': [5, 10, 15, 20, 30, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Decision tree are: {'max_depth': 6, 'min_samples_split': 10}\n",
      "Decision Tree - Train R-squared 0.9603418424143736\n",
      "Decision Tree - Test R-squared 0.8467637959802707\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for Decision tree are:', reg.best_params_)\n",
    "print('Decision Tree - Train R-squared', reg.score(X_train, y_train))\n",
    "print('Decision Tree - Test R-squared', reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10% or lesser Difference': 6994,\n",
       " '25% or lesser Difference': 15,\n",
       " '50% or lesser Difference': 2,\n",
       " '75% or lesser Difference': 0,\n",
       " 'Greater than or Equal to 100%': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_diff(y_train, reg.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10% or lesser Difference': 985,\n",
       " '25% or lesser Difference': 3,\n",
       " '50% or lesser Difference': 0,\n",
       " '75% or lesser Difference': 0,\n",
       " 'Greater than or Equal to 100%': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_diff(y_test, reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth':[5,6,7,8,10,12],'min_samples_split':[5,10,15,20,30] }\n",
    "regressor = RandomForestRegressor(random_state=0, n_estimators= 150, n_jobs=-1)\n",
    "reg = GridSearchCV(regressor, parameters, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=-1,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [5, 6, 7, 8, 10, 12], 'min_samples_split': [5, 10, 15, 20, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Random Forest are: {'max_depth': 5, 'min_samples_split': 5}\n",
      "Random Forest - Train R-squared 0.9538001964306466\n",
      "Random Forest - Test R-squared 0.8664418478729164\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for Random Forest are:', reg.best_params_)\n",
    "print('Random Forest - Train R-squared', reg.score(X_train, y_train))\n",
    "print('Random Forest - Test R-squared', reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10% or lesser Difference': 6966,\n",
       " '25% or lesser Difference': 40,\n",
       " '50% or lesser Difference': 3,\n",
       " '75% or lesser Difference': 2,\n",
       " 'Greater than or Equal to 100%': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_diff(y_train, reg.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10% or lesser Difference': 984,\n",
       " '25% or lesser Difference': 4,\n",
       " '50% or lesser Difference': 0,\n",
       " '75% or lesser Difference': 0,\n",
       " 'Greater than or Equal to 100%': 0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_diff(y_test, reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'learning_rate':[0.001,0.01,0.1,.3,.5],\n",
    "              'n_estimators':[50,100], 'loss': ['square', 'linear', 'exponential'],\n",
    "             'base_estimator__max_depth': [1,2,3],\n",
    "             'base_estimator__min_samples_split': [3,5,10,20]}\n",
    "regressor = AdaBoostRegressor(DecisionTreeRegressor( random_state=0, min_samples_split= 5))\n",
    "reg = GridSearchCV(regressor, parameters, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=AdaBoostRegressor(base_estimator=DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=5, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=0, splitter='best'),\n",
       "         learning_rate=1.0, loss='linear', n_estimators=50,\n",
       "         random_state=None),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'learning_rate': [0.001, 0.01, 0.1, 0.3, 0.5], 'n_estimators': [50, 100], 'loss': ['square', 'linear', 'exponential'], 'base_estimator__max_depth': [1, 2, 3], 'base_estimator__min_samples_split': [3, 5, 10, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Adaboost are: {'base_estimator__max_depth': 3, 'base_estimator__min_samples_split': 10, 'learning_rate': 0.5, 'loss': 'exponential', 'n_estimators': 50}\n",
      "AdaBoost - Train R-squared 0.9388106536209241\n",
      "AdaBoost - Test R-squared 0.8590044003407089\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters for Adaboost are:', reg.best_params_)\n",
    "print('AdaBoost - Train R-squared', reg.score(X_train, y_train))\n",
    "print('AdaBoost - Test R-squared', reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10% or lesser Difference': 3331,\n",
       " '25% or lesser Difference': 2016,\n",
       " '50% or lesser Difference': 1189,\n",
       " '75% or lesser Difference': 287,\n",
       " 'Greater than or Equal to 100%': 188}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_diff(y_train, reg.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10% or lesser Difference': 648,\n",
       " '25% or lesser Difference': 222,\n",
       " '50% or lesser Difference': 86,\n",
       " '75% or lesser Difference': 18,\n",
       " 'Greater than or Equal to 100%': 14}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_diff(y_test, reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, it seems like the Random Forest Regressor has performed the best out of the lot. I observe a fairly high training Rsquared, and a similarly high Test Rsquared. Since the best parametrs are chosen using gridsearch using 5 fold cross validation, we can be positive that the model is not overfitting.  \n",
    "\n",
    "The linear regression with a lasso penalty also preforms fairly well on paper, However, upon observing predictions, I see that the model is actually predicting negative values for hammer price. This is probably due to the fact that the model tries to model the data linearly and tries to fit larget values, thereby sloping the line significantly and predicting negative values for lower values items.\n",
    "\n",
    "Along with this, we also see from the other metric(percentage difference) that most of the values are within 10% difference of the original values in case of the random forest model which is very valuable in determining our confidence in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I run the Random Forest Model again on the 'Future' Data and write the 'Future' Predictions as a CSV file along with the original features that were given. Its not possible to evaluate how good/bad these predictions are as we do not have access to the actual values for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also tried the different models with different features, Initially I had some features that did not add predictive power to the model. Along with that, the feature transformations that I did helped a lot as it gave me a huge boost in metrics. I do not have all the different combinations of features that I used to keep the document short and legible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regressor using best parameters found in gridsearch\n",
    "regressor = RandomForestRegressor(random_state=0, n_estimators= 150, \n",
    "                                  n_jobs=-1,max_depth = 5, min_samples_split = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=5,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=-1,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Train R-squared 0.9538001964306466\n",
      "Random Forest - Test R-squared 0.8664418478729164\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest - Train R-squared', regressor.score(X_train, y_train))\n",
    "print('Random Forest - Test R-squared', regressor.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10% or lesser Difference': 6966,\n",
       " '25% or lesser Difference': 40,\n",
       " '50% or lesser Difference': 3,\n",
       " '75% or lesser Difference': 2,\n",
       " 'Greater than or Equal to 100%': 0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_diff(y_train, regressor.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10% or lesser Difference': 984,\n",
       " '25% or lesser Difference': 4,\n",
       " '50% or lesser Difference': 0,\n",
       " '75% or lesser Difference': 0,\n",
       " 'Greater than or Equal to 100%': 0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_diff(y_test, regressor.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I am showing the correlation between actual values and the predicted values. I notice a very high correlation between the two, this alone is probably not the best metric to track, however, this along with other metrics makes a good case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.977686</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  1.000000  0.977686\n",
       "1  0.977686  1.000000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.corrcoef(y_train,regressor.predict(X_train) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.932686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.932686</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  1.000000  0.932686\n",
       "1  0.932686  1.000000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.corrcoef(y_test,regressor.predict(X_test) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting Values for future auctions\n",
    "predicted_hammer_price = regressor.predict(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging with original data and writing as csv\n",
    "future_data = picasso.loc[future.index]\n",
    "predictions = pd.DataFrame(predicted_hammer_price).set_index(future_data.index) \n",
    "predictions.columns = ['predicted_hammer_price']\n",
    "pd.concat([future_data, predictions], axis = 1).to_csv('artists/picasso_future_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one interesting thing that I notice for the future predictions, artwork that has a smaller estimated/reah sale value, the model tends to predict a higher price, i.e. the minimum price the model predicts is higher than the actual min price. Also, below a threshold, the model predicts the same min value for different data points. This is because the model is trying harder to fit larger values (in 100's of thousands and Millions) than to care about lower values. This causes the random forest to make similar decisions for all values below a certain threshold."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
